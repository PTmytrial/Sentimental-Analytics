{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PTmytrial/Sentimental-Analytics/blob/main/Tokenization_Challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization Challenges"
      ],
      "metadata": {
        "id": "hqrwWxnvePQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is more challenging than you might think.\n",
        "\n",
        "Do we just split our text on non-alphanumeric characters? What about:\n",
        "\n",
        "* hyphens -- can't, won't\n",
        "* mixed words -- Yahoo!\n",
        "* emails and urls -- bob@website.com, www.senecacollege.ca\n",
        "* phone numbers -- 999-999-9999\n",
        "* abbreviations -- U.S.A.\n",
        "* emoticons -- :)\n",
        "\n",
        "What should be considered a token? Should these examples be multiple tokens or a single token?\n",
        "* United States\n",
        "* end of day\n",
        "* is not\n",
        "\n",
        "And should some words be treated as the same? What about:\n",
        "* \"Hello\" and \"hello\" and \"HELLO\"\n",
        "* \"NY\" and \"New York\"\n",
        "* \"ski\" and \"skiing\"\n",
        "* \"good\" and \"great\"\n",
        "\n",
        "There is no right answer to these questions and no single best way to tokenize."
      ],
      "metadata": {
        "id": "W8oIo11ReQkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regular Expressions"
      ],
      "metadata": {
        "id": "oGIxSSi7kfVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A regular expression is a way of describing a pattern that you want to find in text. Here are some examples of regular expressions and their meaning:\n",
        "* `[0-9]` -- any number\n",
        "* `[A-Z]` -- any capital letter\n",
        "* `[A-Z]+` -- any sequence of one or more capital letters\n",
        "* `^A` -- any character that is not a capital A\n",
        "\n",
        "We can use regular expressions to tokenize our text.\n",
        "\n",
        "Regular expressions are almost like a language of themselves. They are implemented in many different programming languages, including Python. Although regular expressions are fairly consistent across languages, you should be aware that there are sometimes small differences.\n",
        "\n",
        "A great resource for visualizing and testing-out regular expressions is [regex101](https://regex101.com/). We will be using this during class. Make sure to choose the Python flavour."
      ],
      "metadata": {
        "id": "ZlczpTpybSjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import re\n",
        "# re.findall()"
      ],
      "metadata": {
        "id": "v7--GthOkg8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "doc = '''Typhoon Yagi, known in the Philippines as Severe Tropical Storm Enteng, was a powerful and destructive tropical cyclone which impacted the Philippines, China, Vietnam and Laos in early September 2024. Yagi, which means goat or the constellation of Capricornus in Japanese, is the eleventh named storm, the first violent typhoon and the first Category 5 storm of the annual typhoon season. It was one of the most intense typhoons ever to strike Northern Vietnam, the strongest typhoon to strike Hainan during the meteorological autumn and the strongest since Rammasun in 2014. It is one of the only four Category 5 super typhoons recorded in the South China Sea, alongside Pamela in 1954, Rammasun in 2014 and Rai in 2021.'''\n",
        "\n",
        "re.findall(\"[A-Za-z0-9]+\", doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L29g1JNXXWET",
        "outputId": "dc00297d-4c65-4e9e-f681-6ece1001b935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Typhoon',\n",
              " 'Yagi',\n",
              " 'known',\n",
              " 'in',\n",
              " 'the',\n",
              " 'Philippines',\n",
              " 'as',\n",
              " 'Severe',\n",
              " 'Tropical',\n",
              " 'Storm',\n",
              " 'Enteng',\n",
              " 'was',\n",
              " 'a',\n",
              " 'powerful',\n",
              " 'and',\n",
              " 'destructive',\n",
              " 'tropical',\n",
              " 'cyclone',\n",
              " 'which',\n",
              " 'impacted',\n",
              " 'the',\n",
              " 'Philippines',\n",
              " 'China',\n",
              " 'Vietnam',\n",
              " 'and',\n",
              " 'Laos',\n",
              " 'in',\n",
              " 'early',\n",
              " 'September',\n",
              " '2024',\n",
              " 'Yagi',\n",
              " 'which',\n",
              " 'means',\n",
              " 'goat',\n",
              " 'or',\n",
              " 'the',\n",
              " 'constellation',\n",
              " 'of',\n",
              " 'Capricornus',\n",
              " 'in',\n",
              " 'Japanese',\n",
              " 'is',\n",
              " 'the',\n",
              " 'eleventh',\n",
              " 'named',\n",
              " 'storm',\n",
              " 'the',\n",
              " 'first',\n",
              " 'violent',\n",
              " 'typhoon',\n",
              " 'and',\n",
              " 'the',\n",
              " 'first',\n",
              " 'Category',\n",
              " '5',\n",
              " 'storm',\n",
              " 'of',\n",
              " 'the',\n",
              " 'annual',\n",
              " 'typhoon',\n",
              " 'season',\n",
              " 'It',\n",
              " 'was',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'most',\n",
              " 'intense',\n",
              " 'typhoons',\n",
              " 'ever',\n",
              " 'to',\n",
              " 'strike',\n",
              " 'Northern',\n",
              " 'Vietnam',\n",
              " 'the',\n",
              " 'strongest',\n",
              " 'typhoon',\n",
              " 'to',\n",
              " 'strike',\n",
              " 'Hainan',\n",
              " 'during',\n",
              " 'the',\n",
              " 'meteorological',\n",
              " 'autumn',\n",
              " 'and',\n",
              " 'the',\n",
              " 'strongest',\n",
              " 'since',\n",
              " 'Rammasun',\n",
              " 'in',\n",
              " '2014',\n",
              " 'It',\n",
              " 'is',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'only',\n",
              " 'four',\n",
              " 'Category',\n",
              " '5',\n",
              " 'super',\n",
              " 'typhoons',\n",
              " 'recorded',\n",
              " 'in',\n",
              " 'the',\n",
              " 'South',\n",
              " 'China',\n",
              " 'Sea',\n",
              " 'alongside',\n",
              " 'Pamela',\n",
              " 'in',\n",
              " '1954',\n",
              " 'Rammasun',\n",
              " 'in',\n",
              " '2014',\n",
              " 'and',\n",
              " 'Rai',\n",
              " 'in',\n",
              " '2021']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization and Stemming"
      ],
      "metadata": {
        "id": "M4BYV0CRknGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is the process of reducing words to their base or root by removing suffixes and prefixes. For example:\n",
        "* \"skiing\" and \"skier\" both become \"ski\"\n",
        "* \"walking\" and \"walks\" both become \"walk\"\n",
        "\n",
        "By doing this, we are treating words with the same \"stem\" as if they are the same words. This helps us train our models because it reduces the sparsity of our data. However, it also removes some meaning.\n",
        "\n",
        "Lemmatization is a more general application of the same idea. Instead of removing prefixes and suffixes, lemmatization replaces a word with a word that has a similar meaning: its \"lemma\".\n",
        "\n",
        "For example, \"huge\" might be replaced with \"big\". Again, this helps us address the sparsity of our data, but comes at the cost of removing meaning."
      ],
      "metadata": {
        "id": "lxBaLNGJeHxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmas as a dictionary\n",
        "# replacing tokens with lemmas"
      ],
      "metadata": {
        "id": "p5fZkZ91lnC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas = {\n",
        "    \"skiing\": \"ski\",\n",
        "    \"skier\": \"ski\",\n",
        "    \"walking\": \"walk\",\n",
        "    \"walks\": \"walk\",\n",
        "    \"huge\": \"big\"\n",
        "}\n",
        "\n",
        "document = 'The skiier was skiing on a huge hill!'\n",
        "\n",
        "tokens = re.findall(\"[A-Za-z0-9]+\", document)\n",
        "for i,v in enumerate(tokens):\n",
        "  lowercase_v = v.lower()\n",
        "  if lowercase_v in lemmas:\n",
        "    tokens[i] = lemmas[lowercase_v]\n",
        "  else:\n",
        "    tokens[i] = lowercase_v\n",
        "\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_g5IPJbYyHf",
        "outputId": "ab210051-2414-4cd7-e540-2ad7fca171a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'skiier', 'was', 'ski', 'on', 'a', 'big', 'hill']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop Words"
      ],
      "metadata": {
        "id": "9XfEICp9kp_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words are words that we remove from our analysis because we consider them insignificant. Generally, these are extremely common words like \"a\" and \"the\" that don't add much meaning to text. We remove these primarily for computational reasons: NLP models can be very computationally intensive and removing insignificant words can make our models run faster and use less compute power. However, with computers getting much better and cheaper, the use of stop words is becoming less common."
      ],
      "metadata": {
        "id": "k-5YswEfdWLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing stop words in a list\n",
        "# removing stop words from a list of tokens"
      ],
      "metadata": {
        "id": "R2jwWRo0livb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas = {\n",
        "    \"skiing\": \"ski\",\n",
        "    \"skier\": \"ski\",\n",
        "    \"huge\": \"big\"\n",
        "}\n",
        "\n",
        "document = 'The skiier was skiing on a huge hill!'\n",
        "\n",
        "tokens = re.findall(\"[A-Za-z0-9]+\", document)\n",
        "\n",
        "for i,v in enumerate(tokens):\n",
        "  lowercase_v = v.lower()\n",
        "  if lowercase_v in lemmas:\n",
        "    tokens[i] = lemmas[lowercase_v]\n",
        "  else:\n",
        "    tokens[i] = lowercase_v\n",
        "\n",
        "stop_words = ['the', 'a', 'on', 'was']\n",
        "\n",
        "print(tokens)\n",
        "for i, v in enumerate(reversed(tokens)):\n",
        "  if v in stop_words:\n",
        "    tokens.remove(v)\n",
        "    print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTib3QitcJVS",
        "outputId": "b72a2355-2a26-4907-fbcd-484b615d405b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'skiier', 'was', 'ski', 'on', 'a', 'big', 'hill']\n",
            "['the', 'skiier', 'was', 'ski', 'on', 'big', 'hill']\n",
            "['the', 'skiier', 'was', 'ski', 'big', 'hill']\n",
            "['the', 'skiier', 'ski', 'big', 'hill']\n",
            "['skiier', 'ski', 'big', 'hill']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZMr_Tsu8d2Td"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}